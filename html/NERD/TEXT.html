<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.7.4" />
<title>NERD.TEXT API documentation</title>
<meta name="description" content="" />
<link href='https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.0/normalize.min.css' rel='stylesheet'>
<link href='https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/8.0.0/sanitize.min.css' rel='stylesheet'>
<link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" rel="stylesheet">
<style>.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{font-weight:bold}#index h4 + ul{margin-bottom:.6em}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>NERD.TEXT</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">#!/usr/bin/env python
# coding: utf-8

from sklearn.feature_extraction.text import CountVectorizer
import numpy as np
from scipy.stats import entropy
import pickle
import os

from flask import Flask
from flask import request
from jinja2 import Template
import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.pipeline import make_pipeline, FeatureUnion
import re

from sklearn.base import TransformerMixin
from sklearn.pipeline import Pipeline
from nltk import pos_tag, word_tokenize
import unicodedata




class ColumnsSelector(TransformerMixin):
    def __init__(self, cols):
        self.cols = cols

    def fit(self, X, y=None):
        return self

    def transform(self, X):
        return X[self.cols]


class DropColumns(TransformerMixin):
    def __init__(self, cols):
        self.cols = cols

    def fit(self, X, y=None):
        return self

    def transform(self, X):
        X = X.drop(columns=self.cols)
        return X


class ToDense(TransformerMixin):
    def __init__(self):
        pass

    def fit(self, X, y=None):
        return self

    def transform(self, X):
        return X.todense()


class DefaultTextFeaturizer(TransformerMixin):
    def __init__(self):
        pass

    def fit(self, X, y=None):
        return self

    @staticmethod
    def _get_nth_token(text, n):
        &#34;&#34;&#34;
        Splits text into tokens and returns the nth token
        Args:
            text: text string
            n: 0 indexed token to return

        Returns:

        &#34;&#34;&#34;
        toks = re.findall(&#39;[\w+\(\),:;\[\]]+&#39;, text)
        if len(toks) &gt; n:
            return toks[n]
        else:
            return &#39;&#39;

    @staticmethod
    def _cleanup_string(text):
        &#34;&#34;&#34;
        Basic text Sanitization
        Args:
            text: text to cleanup

        Returns:

        &#34;&#34;&#34;
        toret = unicodedata.normalize(&#39;NFKD&#39;, text).encode(&#39;ascii&#39;, &#39;ignore&#39;).decode(&#39;ascii&#39;)
        toret = toret.strip()
        toret = re.sub(&#39;[\r\n\t]+&#39;, &#39; &#39;, toret)
        # toks = re.findall(&#39;[\w+\(\),:;\[\]]+&#39;, toret)
        # toret = &#39; &#39;.join(toks)
        toret = re.sub(&#39;[^\w+\(\),:;\[\]\-.|&amp; \t\n/]+&#39;, &#39; &#39;, toret)

        return toret

    @staticmethod
    def _get_pos_string(text, text_len=100):
        if len(text) &lt; text_len:
            tags = pos_tag(word_tokenize(text))
            tags = [t[1] for t in tags]
            return &#39; &#39;.join(tags)
        else:
            return &#39;&#39;

    @staticmethod
    def _is_alpha_and_numeric(string):
        toret = &#39;&#39;
        if string.isdigit():
            toret = &#39;DIGIT&#39;
        elif string.isalpha():
            if string.isupper():
                toret = &#39;ALPHA_UPPER&#39;
            elif string.islower():
                toret = &#39;ALPHA_LOWER&#39;
            else:
                toret = &#39;ALPHA&#39;
        elif len(string) &gt; 0:
            toks = [string[0], string[-1]]
            alphanum = 0
            for tok in toks:
                if tok.isdigit():
                    alphanum += 1
                elif tok.isalpha():
                    alphanum -= 1
            if alphanum == 0:
                toret = &#39;ALPHA_NUM&#39;
        else:
            toret = &#39;EMPTY&#39;

        return toret

    def transform(self, X):
        data = pd.DataFrame(data={&#39;text&#39;: X})
        data.text = data.text.apply(lambda x: DefaultTextFeaturizer._cleanup_string(x))
        data[&#34;pos_string&#34;] = data.text.apply(lambda x: DefaultTextFeaturizer._get_pos_string(x))
        data[&#39;text_feature_text_length&#39;] = data[&#39;text&#39;].apply(lambda x: len(x))
        data[&#39;text_feature_capitals&#39;] = data[&#39;text&#39;].apply(lambda comment: sum(1 for c in comment if c.isupper()))
        data[&#39;text_feature_digits&#39;] = data[&#39;text&#39;].apply(lambda comment: sum(1 for c in comment if c.isdigit()))
        data[&#39;text_feature_caps_vs_length&#39;] = data.apply(
            lambda row: row[&#39;text_feature_capitals&#39;] / (row[&#39;text_feature_text_length&#39;] + 0.001), axis=1)
        data[&#39;text_feature_num_symbols&#39;] = data[&#39;text&#39;].apply(lambda comment: len(re.findall(&#39;\W&#39;, comment)))
        data[&#39;text_feature_num_words&#39;] = data[&#39;text&#39;].apply(lambda comment: len(comment.split()))
        data[&#39;text_feature_num_unique_words&#39;] = data[&#39;text&#39;].apply(lambda comment: len(set(w for w in comment.split())))
        data[&#39;text_feature_words_vs_unique&#39;] = data[&#39;text_feature_num_unique_words&#39;] / (
                data[&#39;text_feature_num_words&#39;] + 0.001)

        data[&#39;text_feature_first_token&#39;] = data[&#39;text&#39;].apply(lambda x: DefaultTextFeaturizer._is_alpha_and_numeric(DefaultTextFeaturizer._get_nth_token(x, 0)))
        data[&#39;text_feature_second_token&#39;] = data[&#39;text&#39;].apply(lambda x: DefaultTextFeaturizer._is_alpha_and_numeric(DefaultTextFeaturizer._get_nth_token(x, 1)))
        data[&#39;text_feature_third_token&#39;] = data[&#39;text&#39;].apply(lambda x: DefaultTextFeaturizer._is_alpha_and_numeric(DefaultTextFeaturizer._get_nth_token(x, 2)))

        data[&#39;text_feature_title_word_count&#39;] = data[&#39;text&#39;].apply(lambda x: sum(1 for c in x.split() if c.istitle()))
        data[&#39;text_feature_title_word_total_word_ratio&#39;] = data[&#39;text_feature_title_word_count&#39;] / (
                data[&#39;text_feature_num_words&#39;] + 0.001)
        data[&#39;text_feature_numeric_tokens&#39;] = data[&#39;text&#39;].apply(lambda x: sum(1 for c in x.split() if c.isdigit()))
        data[&#39;text_feature_capital_tokens&#39;] = data[&#39;text&#39;].apply(lambda x: sum(1 for c in x.split() if c.isupper()))

        return data.drop(columns=[&#39;text&#39;])


class MultiLabelEncoder(TransformerMixin):
    def __init__(self, inplace=False):
        self.inplace = inplace

    def fit(self, X, y=None):
        self.encoder = {}
        self.cols = [c for c in X.columns if X[c].dtype.name == &#39;object&#39;]
        for col in self.cols:
            col_enc = {}
            count = 1
            unique = list(X[col].unique())
            for u in unique:
                col_enc[u] = count
                count += 1
            self.encoder[col] = col_enc

        return self

    def transform(self, X):
        if self.inplace:
            temp = X
        else:
            temp = X.copy()

        for col in self.cols:
            temp[col] = temp[col].apply(lambda x: self.encoder[col].get(x, 0))

        return temp


class BaseTextClassifier:
    &#34;&#34;&#34;
    A utility class for Text Classification
    &#34;&#34;&#34;

    def __init__(self, unlabelled, labelled=None, feature_transformer=None, data_directory=&#39;&#39;):
        &#34;&#34;&#34;
        Initialize with a DataFrame([&#39;text&#39;]) and/or DataFrame([&#39;text&#39;, &#39;class&#39;])
        Args:
            unlabelled: DataFrame([&#39;text&#39;])
            labelled: DataFrame([&#39;text&#39;, &#39;class&#39;])
            feature_transformer: Sklearn transformer to calculate extra features
            data_directory: Default data directory
        &#34;&#34;&#34;
        self.unlabelled = pd.DataFrame(data={&#39;text&#39;: unlabelled})
        self.labelled = labelled

        if self.labelled is not None:
            self.all_data = pd.concat([self.unlabelled, self.labelled])
        else:
            self.all_data = self.unlabelled
            self.all_data[&#39;class&#39;] = np.nan

        # extra feature functions
        if feature_transformer is None:
            self.feature_transformer = DefaultTextFeaturizer()
        else:
            self.feature_transformer = feature_transformer

        self._refresh_text_feature_data()

        self.model = None
        self.data_directory = os.path.join(data_directory, &#39;Text_Classification_Data&#39;)
        os.makedirs(self.data_directory, exist_ok=True)

    def _refresh_text_feature_data(self):
        feature_data = self.feature_transformer.fit_transform(self.all_data[&#39;text&#39;])
        self.feature_columns = list(feature_data.columns)
        for col in feature_data.columns:
            self.all_data[col] = feature_data[col]

    def get_new_random_example(self):
        &#34;&#34;&#34;
        Returns a random example to be tagged. Used to bootstrap the model.
        Returns:

        &#34;&#34;&#34;
        unl = self.all_data[self.all_data[&#39;class&#39;].isna()].index
        self.current_example_index = np.random.choice(unl)
        self.current_example = self.all_data.iloc[self.current_example_index]
        return self.current_example[&#39;text&#39;]

    def query_new_example(self, mode=&#39;entropy&#39;):
        &#34;&#34;&#34;
        Returns a new example based on the chosen active learning strategy.
        Args:
            mode: Active Learning Strategy
                - max (Default)
                - mean
        Returns:

        &#34;&#34;&#34;
        if mode == &#39;entropy&#39;:
            unlab = self.all_data[self.all_data[&#39;class&#39;].isna()]
            unlabelled_idx = unlab.index
            proba = self.model.predict_proba(unlab)
            proba_idx = np.argmax(entropy(proba.T))
            actual_idx = unlabelled_idx[proba_idx]

            self.current_example_index = actual_idx
            self.current_example = self.all_data.iloc[self.current_example_index]
            return self.current_example[&#39;text&#39;]

    def update_model(self):
        &#34;&#34;&#34;
        Updates the model with the currently labelled dataset
        Returns:

        &#34;&#34;&#34;

        if self.model is None:
            self.model = Pipeline([
                (&#39;fu&#39;, FeatureUnion([
                    (&#39;text_vectorizer&#39;,
                     make_pipeline(ColumnsSelector(&#39;text&#39;), CountVectorizer(ngram_range=(1, 2)), ToDense())),
                    (&#39;text_featurizer&#39;, make_pipeline(ColumnsSelector(self.feature_columns), MultiLabelEncoder()))
                ])),
                (&#39;clf&#39;, RandomForestClassifier())
            ])

        lab = self.all_data[self.all_data[&#39;class&#39;].notna()]
        self.model.fit(lab, lab[&#39;class&#39;])

    def save_example(self, data):
        &#34;&#34;&#34;
        Saves the current example with the user tagged data
        Args:
            data: User tagged data. [list of tags].

        Returns:

        &#34;&#34;&#34;
        self.all_data.loc[self.current_example_index, &#39;class&#39;] = data

    def save_data(self, filepath=None):
        &#34;&#34;&#34;
        Saves the labelled data to a file
        Args:
            filepath: file to save the data in a pickle format.

        Returns:

        &#34;&#34;&#34;
        if filepath is None:
            filepath = os.path.join(self.data_directory, &#39;text_classification_data.csv&#39;)
        self.all_data[[&#39;text&#39;, &#39;class&#39;]].to_csv(filepath, index=False)

    def load_data(self, filepath=None):
        &#34;&#34;&#34;
        Loads labelled data from file.|
        Args:
            filepath: file containing pickeled labelled dataset

        Returns:

        &#34;&#34;&#34;
        if filepath is None:
            filepath = os.path.join(self.data_directory, &#39;text_classification_data.csv&#39;)
        self.labelled = pd.read_csv(filepath)
        self.all_data = pd.concat([self.all_data, self.labelled])
        self.all_data.reset_index(inplace=True)
        self._refresh_text_feature_data()

    def add_unlabelled_examples(self, examples):
        &#34;&#34;&#34;
        Append more unlabelled data to dataset
        Args:
            examples: List of strings

        Returns:

        &#34;&#34;&#34;
        new_examples = pd.DataFrame(data={&#39;text&#39;: examples})
        self.all_data = pd.concat([self.all_data, new_examples])
        self.all_data.reset_index(inplace=True)
        self._refresh_text_feature_data()


list_of_colors = &#34;#e6194B, #3cb44b, #ffe119, #4363d8, #f58231, #911eb4, #42d4f4, #f032e6, #bfef45, #fabebe, #469990, #e6beff, #9A6324, #fffac8, #800000, #aaffc3, #808000, #ffd8b1, #000075, #a9a9a9&#34;
list_of_colors = list_of_colors.split(&#39;, &#39;)





class TextClassifier:
    def __init__(self, dataset, unique_tags, data_directory=&#39;&#39;):
        &#34;&#34;&#34;
        Text Classifier from dataset and unique tags
        Args:
            dataset: list of strings
            unique_tags: list of tuples [(identifier, Readable Name)..]
            data_directory: Default data directory
        &#34;&#34;&#34;
        self.unique_tags = unique_tags
        self.tagger = BaseTextClassifier(dataset, data_directory=data_directory)
        self.app = TextClassifier._get_app(self.tagger, self.unique_tags)
        self.utmapping = {t[0]: t[1] for t in self.unique_tags}

    @staticmethod
    def _render_app_template(unique_tags_data):
        &#34;&#34;&#34;
        Tag data in the form
        [
            (tag_id, readable_tag_name)
        ]
        Args:
            unique_tags_data: list of tag tuples

        Returns: html to render

        &#34;&#34;&#34;

        if len(unique_tags_data) &gt; len(list_of_colors):
            return &#34;Too many tags. Add more colors to list_of_colors&#34;

        trainer_path = os.path.join(os.path.dirname(__file__), &#39;html_templates&#39;,
                                    &#39;text_classifier.html&#39;)
        with open(trainer_path) as templ:
            template = Template(templ.read())

        css_classes = []
        for index, item in enumerate(unique_tags_data):
            css_classes.append((item[0], list_of_colors[index]))

        return template.render(css_classes=css_classes, id_color_map=css_classes, tag_controls=unique_tags_data)

    @staticmethod
    def _get_app(tagger, tags):
        app = Flask(__name__)

        @app.route(&#34;/&#34;)
        def base_app():
            return TextClassifier._render_app_template(tags)

        @app.route(&#39;/load_example&#39;)
        def load_example():
            if tagger.model is None:
                example = tagger.get_new_random_example()
            else:
                example = tagger.query_new_example(mode=&#39;entropy&#39;)

            # print(f&#39;Returning example ::: {example[:100]}&#39;)

            return example

        @app.route(&#39;/update_model&#39;)
        def update_model():
            tagger.update_model()
            return &#34;Model Updated Successfully&#34;

        @app.route(&#39;/save_example&#39;, methods=[&#39;POST&#39;])
        def save_example():
            form_data = request.form
            tag = form_data[&#39;tag&#39;]
            tagger.save_example(tag)
            return &#39;Success&#39;

        @app.route(&#39;/save_data&#39;)
        def save_tagged_data():
            print(&#34;save_tagged_data&#34;)
            tagger.save_data()
            return &#39;Data Saved&#39;

        return app

    def start_server(self, port=8000):
        &#34;&#34;&#34;
        Start text classification server at the given port.
        Args:
            port: Port number to bind the server to.

        Returns:

        &#34;&#34;&#34;
        if port:
            self.app.run(port)
        else:
            self.app.run(port=5050)

    def add_unlabelled_examples(self, examples):
        &#34;&#34;&#34;
        Append unlabelled examples to dataset
        Args:
            examples: list of strings

        Returns:

        &#34;&#34;&#34;
        self.tagger.add_unlabelled_examples(examples)

    def save_labelled_examples(self, filepath):
        &#34;&#34;&#34;
        Save labelled examples to file
        Args:
            filepath: destination filename

        Returns:

        &#34;&#34;&#34;
        self.tagger.save_data(filepath)

    def load_labelled_examples(self, filepath):
        &#34;&#34;&#34;
        Load labelled examples to the dataset
        Args:
            filepath: source filename

        Returns:

        &#34;&#34;&#34;
        self.tagger.load_data(filepath)

    def save_model(self, model_filename):
        &#34;&#34;&#34;
        Save classifier model to file
        Args:
            model_filename: destination filename

        Returns:

        &#34;&#34;&#34;
        with open(model_filename, &#39;wb&#39;) as out:
            pickle.dump(self.tagger.model, out)

    def load_model(self, model_filename):
        &#34;&#34;&#34;
        Load classifier model from file
        Args:
            model_filename: source filename

        Returns:

        &#34;&#34;&#34;
        with open(model_filename, &#39;rb&#39;) as inp:
            self.tagger.model = pickle.load(inp)

    def update_model(self):
        &#34;&#34;&#34;
        Updates the model
        Returns:

        &#34;&#34;&#34;
        self.tagger.update_model()


if __name__ == &#39;__main__&#39;:
    # Unique Tags / Classes
    # some text
    tags = [
        (&#34;Address&#34;, &#34;Address&#34;),
        (&#34;Other&#34;, &#34;Non Address&#34;),
    ]</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="NERD.TEXT.BaseTextClassifier"><code class="flex name class">
<span>class <span class="ident">BaseTextClassifier</span></span>
<span>(</span><span>unlabelled, labelled=None, feature_transformer=None, data_directory='')</span>
</code></dt>
<dd>
<section class="desc"><p>A utility class for Text Classification</p>
<p>Initialize with a DataFrame(['text']) and/or DataFrame(['text', 'class'])</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>unlabelled</code></strong></dt>
<dd>DataFrame(['text'])</dd>
<dt><strong><code>labelled</code></strong></dt>
<dd>DataFrame(['text', 'class'])</dd>
<dt><strong><code>feature_transformer</code></strong></dt>
<dd>Sklearn transformer to calculate extra features</dd>
<dt><strong><code>data_directory</code></strong></dt>
<dd>Default data directory</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class BaseTextClassifier:
    &#34;&#34;&#34;
    A utility class for Text Classification
    &#34;&#34;&#34;

    def __init__(self, unlabelled, labelled=None, feature_transformer=None, data_directory=&#39;&#39;):
        &#34;&#34;&#34;
        Initialize with a DataFrame([&#39;text&#39;]) and/or DataFrame([&#39;text&#39;, &#39;class&#39;])
        Args:
            unlabelled: DataFrame([&#39;text&#39;])
            labelled: DataFrame([&#39;text&#39;, &#39;class&#39;])
            feature_transformer: Sklearn transformer to calculate extra features
            data_directory: Default data directory
        &#34;&#34;&#34;
        self.unlabelled = pd.DataFrame(data={&#39;text&#39;: unlabelled})
        self.labelled = labelled

        if self.labelled is not None:
            self.all_data = pd.concat([self.unlabelled, self.labelled])
        else:
            self.all_data = self.unlabelled
            self.all_data[&#39;class&#39;] = np.nan

        # extra feature functions
        if feature_transformer is None:
            self.feature_transformer = DefaultTextFeaturizer()
        else:
            self.feature_transformer = feature_transformer

        self._refresh_text_feature_data()

        self.model = None
        self.data_directory = os.path.join(data_directory, &#39;Text_Classification_Data&#39;)
        os.makedirs(self.data_directory, exist_ok=True)

    def _refresh_text_feature_data(self):
        feature_data = self.feature_transformer.fit_transform(self.all_data[&#39;text&#39;])
        self.feature_columns = list(feature_data.columns)
        for col in feature_data.columns:
            self.all_data[col] = feature_data[col]

    def get_new_random_example(self):
        &#34;&#34;&#34;
        Returns a random example to be tagged. Used to bootstrap the model.
        Returns:

        &#34;&#34;&#34;
        unl = self.all_data[self.all_data[&#39;class&#39;].isna()].index
        self.current_example_index = np.random.choice(unl)
        self.current_example = self.all_data.iloc[self.current_example_index]
        return self.current_example[&#39;text&#39;]

    def query_new_example(self, mode=&#39;entropy&#39;):
        &#34;&#34;&#34;
        Returns a new example based on the chosen active learning strategy.
        Args:
            mode: Active Learning Strategy
                - max (Default)
                - mean
        Returns:

        &#34;&#34;&#34;
        if mode == &#39;entropy&#39;:
            unlab = self.all_data[self.all_data[&#39;class&#39;].isna()]
            unlabelled_idx = unlab.index
            proba = self.model.predict_proba(unlab)
            proba_idx = np.argmax(entropy(proba.T))
            actual_idx = unlabelled_idx[proba_idx]

            self.current_example_index = actual_idx
            self.current_example = self.all_data.iloc[self.current_example_index]
            return self.current_example[&#39;text&#39;]

    def update_model(self):
        &#34;&#34;&#34;
        Updates the model with the currently labelled dataset
        Returns:

        &#34;&#34;&#34;

        if self.model is None:
            self.model = Pipeline([
                (&#39;fu&#39;, FeatureUnion([
                    (&#39;text_vectorizer&#39;,
                     make_pipeline(ColumnsSelector(&#39;text&#39;), CountVectorizer(ngram_range=(1, 2)), ToDense())),
                    (&#39;text_featurizer&#39;, make_pipeline(ColumnsSelector(self.feature_columns), MultiLabelEncoder()))
                ])),
                (&#39;clf&#39;, RandomForestClassifier())
            ])

        lab = self.all_data[self.all_data[&#39;class&#39;].notna()]
        self.model.fit(lab, lab[&#39;class&#39;])

    def save_example(self, data):
        &#34;&#34;&#34;
        Saves the current example with the user tagged data
        Args:
            data: User tagged data. [list of tags].

        Returns:

        &#34;&#34;&#34;
        self.all_data.loc[self.current_example_index, &#39;class&#39;] = data

    def save_data(self, filepath=None):
        &#34;&#34;&#34;
        Saves the labelled data to a file
        Args:
            filepath: file to save the data in a pickle format.

        Returns:

        &#34;&#34;&#34;
        if filepath is None:
            filepath = os.path.join(self.data_directory, &#39;text_classification_data.csv&#39;)
        self.all_data[[&#39;text&#39;, &#39;class&#39;]].to_csv(filepath, index=False)

    def load_data(self, filepath=None):
        &#34;&#34;&#34;
        Loads labelled data from file.|
        Args:
            filepath: file containing pickeled labelled dataset

        Returns:

        &#34;&#34;&#34;
        if filepath is None:
            filepath = os.path.join(self.data_directory, &#39;text_classification_data.csv&#39;)
        self.labelled = pd.read_csv(filepath)
        self.all_data = pd.concat([self.all_data, self.labelled])
        self.all_data.reset_index(inplace=True)
        self._refresh_text_feature_data()

    def add_unlabelled_examples(self, examples):
        &#34;&#34;&#34;
        Append more unlabelled data to dataset
        Args:
            examples: List of strings

        Returns:

        &#34;&#34;&#34;
        new_examples = pd.DataFrame(data={&#39;text&#39;: examples})
        self.all_data = pd.concat([self.all_data, new_examples])
        self.all_data.reset_index(inplace=True)
        self._refresh_text_feature_data()</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="NERD.TEXT.BaseTextClassifier.add_unlabelled_examples"><code class="name flex">
<span>def <span class="ident">add_unlabelled_examples</span></span>(<span>self, examples)</span>
</code></dt>
<dd>
<section class="desc"><p>Append more unlabelled data to dataset</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>examples</code></strong></dt>
<dd>List of strings</dd>
</dl>
<p>Returns:</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def add_unlabelled_examples(self, examples):
    &#34;&#34;&#34;
    Append more unlabelled data to dataset
    Args:
        examples: List of strings

    Returns:

    &#34;&#34;&#34;
    new_examples = pd.DataFrame(data={&#39;text&#39;: examples})
    self.all_data = pd.concat([self.all_data, new_examples])
    self.all_data.reset_index(inplace=True)
    self._refresh_text_feature_data()</code></pre>
</details>
</dd>
<dt id="NERD.TEXT.BaseTextClassifier.get_new_random_example"><code class="name flex">
<span>def <span class="ident">get_new_random_example</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Returns a random example to be tagged. Used to bootstrap the model.
Returns:</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_new_random_example(self):
    &#34;&#34;&#34;
    Returns a random example to be tagged. Used to bootstrap the model.
    Returns:

    &#34;&#34;&#34;
    unl = self.all_data[self.all_data[&#39;class&#39;].isna()].index
    self.current_example_index = np.random.choice(unl)
    self.current_example = self.all_data.iloc[self.current_example_index]
    return self.current_example[&#39;text&#39;]</code></pre>
</details>
</dd>
<dt id="NERD.TEXT.BaseTextClassifier.load_data"><code class="name flex">
<span>def <span class="ident">load_data</span></span>(<span>self, filepath=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Loads labelled data from file.|</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>filepath</code></strong></dt>
<dd>file containing pickeled labelled dataset</dd>
</dl>
<p>Returns:</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def load_data(self, filepath=None):
    &#34;&#34;&#34;
    Loads labelled data from file.|
    Args:
        filepath: file containing pickeled labelled dataset

    Returns:

    &#34;&#34;&#34;
    if filepath is None:
        filepath = os.path.join(self.data_directory, &#39;text_classification_data.csv&#39;)
    self.labelled = pd.read_csv(filepath)
    self.all_data = pd.concat([self.all_data, self.labelled])
    self.all_data.reset_index(inplace=True)
    self._refresh_text_feature_data()</code></pre>
</details>
</dd>
<dt id="NERD.TEXT.BaseTextClassifier.query_new_example"><code class="name flex">
<span>def <span class="ident">query_new_example</span></span>(<span>self, mode='entropy')</span>
</code></dt>
<dd>
<section class="desc"><p>Returns a new example based on the chosen active learning strategy.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>mode</code></strong></dt>
<dd>Active Learning Strategy
- max (Default)
- mean</dd>
</dl>
<p>Returns:</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def query_new_example(self, mode=&#39;entropy&#39;):
    &#34;&#34;&#34;
    Returns a new example based on the chosen active learning strategy.
    Args:
        mode: Active Learning Strategy
            - max (Default)
            - mean
    Returns:

    &#34;&#34;&#34;
    if mode == &#39;entropy&#39;:
        unlab = self.all_data[self.all_data[&#39;class&#39;].isna()]
        unlabelled_idx = unlab.index
        proba = self.model.predict_proba(unlab)
        proba_idx = np.argmax(entropy(proba.T))
        actual_idx = unlabelled_idx[proba_idx]

        self.current_example_index = actual_idx
        self.current_example = self.all_data.iloc[self.current_example_index]
        return self.current_example[&#39;text&#39;]</code></pre>
</details>
</dd>
<dt id="NERD.TEXT.BaseTextClassifier.save_data"><code class="name flex">
<span>def <span class="ident">save_data</span></span>(<span>self, filepath=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Saves the labelled data to a file</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>filepath</code></strong></dt>
<dd>file to save the data in a pickle format.</dd>
</dl>
<p>Returns:</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def save_data(self, filepath=None):
    &#34;&#34;&#34;
    Saves the labelled data to a file
    Args:
        filepath: file to save the data in a pickle format.

    Returns:

    &#34;&#34;&#34;
    if filepath is None:
        filepath = os.path.join(self.data_directory, &#39;text_classification_data.csv&#39;)
    self.all_data[[&#39;text&#39;, &#39;class&#39;]].to_csv(filepath, index=False)</code></pre>
</details>
</dd>
<dt id="NERD.TEXT.BaseTextClassifier.save_example"><code class="name flex">
<span>def <span class="ident">save_example</span></span>(<span>self, data)</span>
</code></dt>
<dd>
<section class="desc"><p>Saves the current example with the user tagged data</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>data</code></strong></dt>
<dd>User tagged data. [list of tags].</dd>
</dl>
<p>Returns:</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def save_example(self, data):
    &#34;&#34;&#34;
    Saves the current example with the user tagged data
    Args:
        data: User tagged data. [list of tags].

    Returns:

    &#34;&#34;&#34;
    self.all_data.loc[self.current_example_index, &#39;class&#39;] = data</code></pre>
</details>
</dd>
<dt id="NERD.TEXT.BaseTextClassifier.update_model"><code class="name flex">
<span>def <span class="ident">update_model</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Updates the model with the currently labelled dataset
Returns:</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def update_model(self):
    &#34;&#34;&#34;
    Updates the model with the currently labelled dataset
    Returns:

    &#34;&#34;&#34;

    if self.model is None:
        self.model = Pipeline([
            (&#39;fu&#39;, FeatureUnion([
                (&#39;text_vectorizer&#39;,
                 make_pipeline(ColumnsSelector(&#39;text&#39;), CountVectorizer(ngram_range=(1, 2)), ToDense())),
                (&#39;text_featurizer&#39;, make_pipeline(ColumnsSelector(self.feature_columns), MultiLabelEncoder()))
            ])),
            (&#39;clf&#39;, RandomForestClassifier())
        ])

    lab = self.all_data[self.all_data[&#39;class&#39;].notna()]
    self.model.fit(lab, lab[&#39;class&#39;])</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="NERD.TEXT.ColumnsSelector"><code class="flex name class">
<span>class <span class="ident">ColumnsSelector</span></span>
<span>(</span><span>cols)</span>
</code></dt>
<dd>
<section class="desc"><p>Mixin class for all transformers in scikit-learn.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ColumnsSelector(TransformerMixin):
    def __init__(self, cols):
        self.cols = cols

    def fit(self, X, y=None):
        return self

    def transform(self, X):
        return X[self.cols]</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>sklearn.base.TransformerMixin</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="NERD.TEXT.ColumnsSelector.fit"><code class="name flex">
<span>def <span class="ident">fit</span></span>(<span>self, X, y=None)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def fit(self, X, y=None):
    return self</code></pre>
</details>
</dd>
<dt id="NERD.TEXT.ColumnsSelector.transform"><code class="name flex">
<span>def <span class="ident">transform</span></span>(<span>self, X)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def transform(self, X):
    return X[self.cols]</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="NERD.TEXT.DefaultTextFeaturizer"><code class="flex name class">
<span>class <span class="ident">DefaultTextFeaturizer</span></span>
</code></dt>
<dd>
<section class="desc"><p>Mixin class for all transformers in scikit-learn.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class DefaultTextFeaturizer(TransformerMixin):
    def __init__(self):
        pass

    def fit(self, X, y=None):
        return self

    @staticmethod
    def _get_nth_token(text, n):
        &#34;&#34;&#34;
        Splits text into tokens and returns the nth token
        Args:
            text: text string
            n: 0 indexed token to return

        Returns:

        &#34;&#34;&#34;
        toks = re.findall(&#39;[\w+\(\),:;\[\]]+&#39;, text)
        if len(toks) &gt; n:
            return toks[n]
        else:
            return &#39;&#39;

    @staticmethod
    def _cleanup_string(text):
        &#34;&#34;&#34;
        Basic text Sanitization
        Args:
            text: text to cleanup

        Returns:

        &#34;&#34;&#34;
        toret = unicodedata.normalize(&#39;NFKD&#39;, text).encode(&#39;ascii&#39;, &#39;ignore&#39;).decode(&#39;ascii&#39;)
        toret = toret.strip()
        toret = re.sub(&#39;[\r\n\t]+&#39;, &#39; &#39;, toret)
        # toks = re.findall(&#39;[\w+\(\),:;\[\]]+&#39;, toret)
        # toret = &#39; &#39;.join(toks)
        toret = re.sub(&#39;[^\w+\(\),:;\[\]\-.|&amp; \t\n/]+&#39;, &#39; &#39;, toret)

        return toret

    @staticmethod
    def _get_pos_string(text, text_len=100):
        if len(text) &lt; text_len:
            tags = pos_tag(word_tokenize(text))
            tags = [t[1] for t in tags]
            return &#39; &#39;.join(tags)
        else:
            return &#39;&#39;

    @staticmethod
    def _is_alpha_and_numeric(string):
        toret = &#39;&#39;
        if string.isdigit():
            toret = &#39;DIGIT&#39;
        elif string.isalpha():
            if string.isupper():
                toret = &#39;ALPHA_UPPER&#39;
            elif string.islower():
                toret = &#39;ALPHA_LOWER&#39;
            else:
                toret = &#39;ALPHA&#39;
        elif len(string) &gt; 0:
            toks = [string[0], string[-1]]
            alphanum = 0
            for tok in toks:
                if tok.isdigit():
                    alphanum += 1
                elif tok.isalpha():
                    alphanum -= 1
            if alphanum == 0:
                toret = &#39;ALPHA_NUM&#39;
        else:
            toret = &#39;EMPTY&#39;

        return toret

    def transform(self, X):
        data = pd.DataFrame(data={&#39;text&#39;: X})
        data.text = data.text.apply(lambda x: DefaultTextFeaturizer._cleanup_string(x))
        data[&#34;pos_string&#34;] = data.text.apply(lambda x: DefaultTextFeaturizer._get_pos_string(x))
        data[&#39;text_feature_text_length&#39;] = data[&#39;text&#39;].apply(lambda x: len(x))
        data[&#39;text_feature_capitals&#39;] = data[&#39;text&#39;].apply(lambda comment: sum(1 for c in comment if c.isupper()))
        data[&#39;text_feature_digits&#39;] = data[&#39;text&#39;].apply(lambda comment: sum(1 for c in comment if c.isdigit()))
        data[&#39;text_feature_caps_vs_length&#39;] = data.apply(
            lambda row: row[&#39;text_feature_capitals&#39;] / (row[&#39;text_feature_text_length&#39;] + 0.001), axis=1)
        data[&#39;text_feature_num_symbols&#39;] = data[&#39;text&#39;].apply(lambda comment: len(re.findall(&#39;\W&#39;, comment)))
        data[&#39;text_feature_num_words&#39;] = data[&#39;text&#39;].apply(lambda comment: len(comment.split()))
        data[&#39;text_feature_num_unique_words&#39;] = data[&#39;text&#39;].apply(lambda comment: len(set(w for w in comment.split())))
        data[&#39;text_feature_words_vs_unique&#39;] = data[&#39;text_feature_num_unique_words&#39;] / (
                data[&#39;text_feature_num_words&#39;] + 0.001)

        data[&#39;text_feature_first_token&#39;] = data[&#39;text&#39;].apply(lambda x: DefaultTextFeaturizer._is_alpha_and_numeric(DefaultTextFeaturizer._get_nth_token(x, 0)))
        data[&#39;text_feature_second_token&#39;] = data[&#39;text&#39;].apply(lambda x: DefaultTextFeaturizer._is_alpha_and_numeric(DefaultTextFeaturizer._get_nth_token(x, 1)))
        data[&#39;text_feature_third_token&#39;] = data[&#39;text&#39;].apply(lambda x: DefaultTextFeaturizer._is_alpha_and_numeric(DefaultTextFeaturizer._get_nth_token(x, 2)))

        data[&#39;text_feature_title_word_count&#39;] = data[&#39;text&#39;].apply(lambda x: sum(1 for c in x.split() if c.istitle()))
        data[&#39;text_feature_title_word_total_word_ratio&#39;] = data[&#39;text_feature_title_word_count&#39;] / (
                data[&#39;text_feature_num_words&#39;] + 0.001)
        data[&#39;text_feature_numeric_tokens&#39;] = data[&#39;text&#39;].apply(lambda x: sum(1 for c in x.split() if c.isdigit()))
        data[&#39;text_feature_capital_tokens&#39;] = data[&#39;text&#39;].apply(lambda x: sum(1 for c in x.split() if c.isupper()))

        return data.drop(columns=[&#39;text&#39;])</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>sklearn.base.TransformerMixin</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="NERD.TEXT.DefaultTextFeaturizer.fit"><code class="name flex">
<span>def <span class="ident">fit</span></span>(<span>self, X, y=None)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def fit(self, X, y=None):
    return self</code></pre>
</details>
</dd>
<dt id="NERD.TEXT.DefaultTextFeaturizer.transform"><code class="name flex">
<span>def <span class="ident">transform</span></span>(<span>self, X)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def transform(self, X):
    data = pd.DataFrame(data={&#39;text&#39;: X})
    data.text = data.text.apply(lambda x: DefaultTextFeaturizer._cleanup_string(x))
    data[&#34;pos_string&#34;] = data.text.apply(lambda x: DefaultTextFeaturizer._get_pos_string(x))
    data[&#39;text_feature_text_length&#39;] = data[&#39;text&#39;].apply(lambda x: len(x))
    data[&#39;text_feature_capitals&#39;] = data[&#39;text&#39;].apply(lambda comment: sum(1 for c in comment if c.isupper()))
    data[&#39;text_feature_digits&#39;] = data[&#39;text&#39;].apply(lambda comment: sum(1 for c in comment if c.isdigit()))
    data[&#39;text_feature_caps_vs_length&#39;] = data.apply(
        lambda row: row[&#39;text_feature_capitals&#39;] / (row[&#39;text_feature_text_length&#39;] + 0.001), axis=1)
    data[&#39;text_feature_num_symbols&#39;] = data[&#39;text&#39;].apply(lambda comment: len(re.findall(&#39;\W&#39;, comment)))
    data[&#39;text_feature_num_words&#39;] = data[&#39;text&#39;].apply(lambda comment: len(comment.split()))
    data[&#39;text_feature_num_unique_words&#39;] = data[&#39;text&#39;].apply(lambda comment: len(set(w for w in comment.split())))
    data[&#39;text_feature_words_vs_unique&#39;] = data[&#39;text_feature_num_unique_words&#39;] / (
            data[&#39;text_feature_num_words&#39;] + 0.001)

    data[&#39;text_feature_first_token&#39;] = data[&#39;text&#39;].apply(lambda x: DefaultTextFeaturizer._is_alpha_and_numeric(DefaultTextFeaturizer._get_nth_token(x, 0)))
    data[&#39;text_feature_second_token&#39;] = data[&#39;text&#39;].apply(lambda x: DefaultTextFeaturizer._is_alpha_and_numeric(DefaultTextFeaturizer._get_nth_token(x, 1)))
    data[&#39;text_feature_third_token&#39;] = data[&#39;text&#39;].apply(lambda x: DefaultTextFeaturizer._is_alpha_and_numeric(DefaultTextFeaturizer._get_nth_token(x, 2)))

    data[&#39;text_feature_title_word_count&#39;] = data[&#39;text&#39;].apply(lambda x: sum(1 for c in x.split() if c.istitle()))
    data[&#39;text_feature_title_word_total_word_ratio&#39;] = data[&#39;text_feature_title_word_count&#39;] / (
            data[&#39;text_feature_num_words&#39;] + 0.001)
    data[&#39;text_feature_numeric_tokens&#39;] = data[&#39;text&#39;].apply(lambda x: sum(1 for c in x.split() if c.isdigit()))
    data[&#39;text_feature_capital_tokens&#39;] = data[&#39;text&#39;].apply(lambda x: sum(1 for c in x.split() if c.isupper()))

    return data.drop(columns=[&#39;text&#39;])</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="NERD.TEXT.DropColumns"><code class="flex name class">
<span>class <span class="ident">DropColumns</span></span>
<span>(</span><span>cols)</span>
</code></dt>
<dd>
<section class="desc"><p>Mixin class for all transformers in scikit-learn.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class DropColumns(TransformerMixin):
    def __init__(self, cols):
        self.cols = cols

    def fit(self, X, y=None):
        return self

    def transform(self, X):
        X = X.drop(columns=self.cols)
        return X</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>sklearn.base.TransformerMixin</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="NERD.TEXT.DropColumns.fit"><code class="name flex">
<span>def <span class="ident">fit</span></span>(<span>self, X, y=None)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def fit(self, X, y=None):
    return self</code></pre>
</details>
</dd>
<dt id="NERD.TEXT.DropColumns.transform"><code class="name flex">
<span>def <span class="ident">transform</span></span>(<span>self, X)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def transform(self, X):
    X = X.drop(columns=self.cols)
    return X</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="NERD.TEXT.MultiLabelEncoder"><code class="flex name class">
<span>class <span class="ident">MultiLabelEncoder</span></span>
<span>(</span><span>inplace=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Mixin class for all transformers in scikit-learn.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class MultiLabelEncoder(TransformerMixin):
    def __init__(self, inplace=False):
        self.inplace = inplace

    def fit(self, X, y=None):
        self.encoder = {}
        self.cols = [c for c in X.columns if X[c].dtype.name == &#39;object&#39;]
        for col in self.cols:
            col_enc = {}
            count = 1
            unique = list(X[col].unique())
            for u in unique:
                col_enc[u] = count
                count += 1
            self.encoder[col] = col_enc

        return self

    def transform(self, X):
        if self.inplace:
            temp = X
        else:
            temp = X.copy()

        for col in self.cols:
            temp[col] = temp[col].apply(lambda x: self.encoder[col].get(x, 0))

        return temp</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>sklearn.base.TransformerMixin</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="NERD.TEXT.MultiLabelEncoder.fit"><code class="name flex">
<span>def <span class="ident">fit</span></span>(<span>self, X, y=None)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def fit(self, X, y=None):
    self.encoder = {}
    self.cols = [c for c in X.columns if X[c].dtype.name == &#39;object&#39;]
    for col in self.cols:
        col_enc = {}
        count = 1
        unique = list(X[col].unique())
        for u in unique:
            col_enc[u] = count
            count += 1
        self.encoder[col] = col_enc

    return self</code></pre>
</details>
</dd>
<dt id="NERD.TEXT.MultiLabelEncoder.transform"><code class="name flex">
<span>def <span class="ident">transform</span></span>(<span>self, X)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def transform(self, X):
    if self.inplace:
        temp = X
    else:
        temp = X.copy()

    for col in self.cols:
        temp[col] = temp[col].apply(lambda x: self.encoder[col].get(x, 0))

    return temp</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="NERD.TEXT.TextClassifier"><code class="flex name class">
<span>class <span class="ident">TextClassifier</span></span>
<span>(</span><span>dataset, unique_tags, data_directory='')</span>
</code></dt>
<dd>
<section class="desc"><p>Text Classifier from dataset and unique tags</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>dataset</code></strong></dt>
<dd>list of strings</dd>
<dt><strong><code>unique_tags</code></strong></dt>
<dd>list of tuples [(identifier, Readable Name)..]</dd>
<dt><strong><code>data_directory</code></strong></dt>
<dd>Default data directory</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class TextClassifier:
    def __init__(self, dataset, unique_tags, data_directory=&#39;&#39;):
        &#34;&#34;&#34;
        Text Classifier from dataset and unique tags
        Args:
            dataset: list of strings
            unique_tags: list of tuples [(identifier, Readable Name)..]
            data_directory: Default data directory
        &#34;&#34;&#34;
        self.unique_tags = unique_tags
        self.tagger = BaseTextClassifier(dataset, data_directory=data_directory)
        self.app = TextClassifier._get_app(self.tagger, self.unique_tags)
        self.utmapping = {t[0]: t[1] for t in self.unique_tags}

    @staticmethod
    def _render_app_template(unique_tags_data):
        &#34;&#34;&#34;
        Tag data in the form
        [
            (tag_id, readable_tag_name)
        ]
        Args:
            unique_tags_data: list of tag tuples

        Returns: html to render

        &#34;&#34;&#34;

        if len(unique_tags_data) &gt; len(list_of_colors):
            return &#34;Too many tags. Add more colors to list_of_colors&#34;

        trainer_path = os.path.join(os.path.dirname(__file__), &#39;html_templates&#39;,
                                    &#39;text_classifier.html&#39;)
        with open(trainer_path) as templ:
            template = Template(templ.read())

        css_classes = []
        for index, item in enumerate(unique_tags_data):
            css_classes.append((item[0], list_of_colors[index]))

        return template.render(css_classes=css_classes, id_color_map=css_classes, tag_controls=unique_tags_data)

    @staticmethod
    def _get_app(tagger, tags):
        app = Flask(__name__)

        @app.route(&#34;/&#34;)
        def base_app():
            return TextClassifier._render_app_template(tags)

        @app.route(&#39;/load_example&#39;)
        def load_example():
            if tagger.model is None:
                example = tagger.get_new_random_example()
            else:
                example = tagger.query_new_example(mode=&#39;entropy&#39;)

            # print(f&#39;Returning example ::: {example[:100]}&#39;)

            return example

        @app.route(&#39;/update_model&#39;)
        def update_model():
            tagger.update_model()
            return &#34;Model Updated Successfully&#34;

        @app.route(&#39;/save_example&#39;, methods=[&#39;POST&#39;])
        def save_example():
            form_data = request.form
            tag = form_data[&#39;tag&#39;]
            tagger.save_example(tag)
            return &#39;Success&#39;

        @app.route(&#39;/save_data&#39;)
        def save_tagged_data():
            print(&#34;save_tagged_data&#34;)
            tagger.save_data()
            return &#39;Data Saved&#39;

        return app

    def start_server(self, port=8000):
        &#34;&#34;&#34;
        Start text classification server at the given port.
        Args:
            port: Port number to bind the server to.

        Returns:

        &#34;&#34;&#34;
        if port:
            self.app.run(port)
        else:
            self.app.run(port=5050)

    def add_unlabelled_examples(self, examples):
        &#34;&#34;&#34;
        Append unlabelled examples to dataset
        Args:
            examples: list of strings

        Returns:

        &#34;&#34;&#34;
        self.tagger.add_unlabelled_examples(examples)

    def save_labelled_examples(self, filepath):
        &#34;&#34;&#34;
        Save labelled examples to file
        Args:
            filepath: destination filename

        Returns:

        &#34;&#34;&#34;
        self.tagger.save_data(filepath)

    def load_labelled_examples(self, filepath):
        &#34;&#34;&#34;
        Load labelled examples to the dataset
        Args:
            filepath: source filename

        Returns:

        &#34;&#34;&#34;
        self.tagger.load_data(filepath)

    def save_model(self, model_filename):
        &#34;&#34;&#34;
        Save classifier model to file
        Args:
            model_filename: destination filename

        Returns:

        &#34;&#34;&#34;
        with open(model_filename, &#39;wb&#39;) as out:
            pickle.dump(self.tagger.model, out)

    def load_model(self, model_filename):
        &#34;&#34;&#34;
        Load classifier model from file
        Args:
            model_filename: source filename

        Returns:

        &#34;&#34;&#34;
        with open(model_filename, &#39;rb&#39;) as inp:
            self.tagger.model = pickle.load(inp)

    def update_model(self):
        &#34;&#34;&#34;
        Updates the model
        Returns:

        &#34;&#34;&#34;
        self.tagger.update_model()</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="NERD.TEXT.TextClassifier.add_unlabelled_examples"><code class="name flex">
<span>def <span class="ident">add_unlabelled_examples</span></span>(<span>self, examples)</span>
</code></dt>
<dd>
<section class="desc"><p>Append unlabelled examples to dataset</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>examples</code></strong></dt>
<dd>list of strings</dd>
</dl>
<p>Returns:</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def add_unlabelled_examples(self, examples):
    &#34;&#34;&#34;
    Append unlabelled examples to dataset
    Args:
        examples: list of strings

    Returns:

    &#34;&#34;&#34;
    self.tagger.add_unlabelled_examples(examples)</code></pre>
</details>
</dd>
<dt id="NERD.TEXT.TextClassifier.load_labelled_examples"><code class="name flex">
<span>def <span class="ident">load_labelled_examples</span></span>(<span>self, filepath)</span>
</code></dt>
<dd>
<section class="desc"><p>Load labelled examples to the dataset</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>filepath</code></strong></dt>
<dd>source filename</dd>
</dl>
<p>Returns:</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def load_labelled_examples(self, filepath):
    &#34;&#34;&#34;
    Load labelled examples to the dataset
    Args:
        filepath: source filename

    Returns:

    &#34;&#34;&#34;
    self.tagger.load_data(filepath)</code></pre>
</details>
</dd>
<dt id="NERD.TEXT.TextClassifier.load_model"><code class="name flex">
<span>def <span class="ident">load_model</span></span>(<span>self, model_filename)</span>
</code></dt>
<dd>
<section class="desc"><p>Load classifier model from file</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>model_filename</code></strong></dt>
<dd>source filename</dd>
</dl>
<p>Returns:</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def load_model(self, model_filename):
    &#34;&#34;&#34;
    Load classifier model from file
    Args:
        model_filename: source filename

    Returns:

    &#34;&#34;&#34;
    with open(model_filename, &#39;rb&#39;) as inp:
        self.tagger.model = pickle.load(inp)</code></pre>
</details>
</dd>
<dt id="NERD.TEXT.TextClassifier.save_labelled_examples"><code class="name flex">
<span>def <span class="ident">save_labelled_examples</span></span>(<span>self, filepath)</span>
</code></dt>
<dd>
<section class="desc"><p>Save labelled examples to file</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>filepath</code></strong></dt>
<dd>destination filename</dd>
</dl>
<p>Returns:</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def save_labelled_examples(self, filepath):
    &#34;&#34;&#34;
    Save labelled examples to file
    Args:
        filepath: destination filename

    Returns:

    &#34;&#34;&#34;
    self.tagger.save_data(filepath)</code></pre>
</details>
</dd>
<dt id="NERD.TEXT.TextClassifier.save_model"><code class="name flex">
<span>def <span class="ident">save_model</span></span>(<span>self, model_filename)</span>
</code></dt>
<dd>
<section class="desc"><p>Save classifier model to file</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>model_filename</code></strong></dt>
<dd>destination filename</dd>
</dl>
<p>Returns:</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def save_model(self, model_filename):
    &#34;&#34;&#34;
    Save classifier model to file
    Args:
        model_filename: destination filename

    Returns:

    &#34;&#34;&#34;
    with open(model_filename, &#39;wb&#39;) as out:
        pickle.dump(self.tagger.model, out)</code></pre>
</details>
</dd>
<dt id="NERD.TEXT.TextClassifier.start_server"><code class="name flex">
<span>def <span class="ident">start_server</span></span>(<span>self, port=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Start text classification server at the given port.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>port</code></strong></dt>
<dd>Port number to bind the server to.</dd>
</dl>
<p>Returns:</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def start_server(self, port=None):
    &#34;&#34;&#34;
    Start text classification server at the given port.
    Args:
        port: Port number to bind the server to.

    Returns:

    &#34;&#34;&#34;
    if port:
        self.app.run(port)
    else:
        self.app.run(port=5050)</code></pre>
</details>
</dd>
<dt id="NERD.TEXT.TextClassifier.update_model"><code class="name flex">
<span>def <span class="ident">update_model</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Updates the model
Returns:</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def update_model(self):
    &#34;&#34;&#34;
    Updates the model
    Returns:

    &#34;&#34;&#34;
    self.tagger.update_model()</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="NERD.TEXT.ToDense"><code class="flex name class">
<span>class <span class="ident">ToDense</span></span>
</code></dt>
<dd>
<section class="desc"><p>Mixin class for all transformers in scikit-learn.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ToDense(TransformerMixin):
    def __init__(self):
        pass

    def fit(self, X, y=None):
        return self

    def transform(self, X):
        return X.todense()</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>sklearn.base.TransformerMixin</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="NERD.TEXT.ToDense.fit"><code class="name flex">
<span>def <span class="ident">fit</span></span>(<span>self, X, y=None)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def fit(self, X, y=None):
    return self</code></pre>
</details>
</dd>
<dt id="NERD.TEXT.ToDense.transform"><code class="name flex">
<span>def <span class="ident">transform</span></span>(<span>self, X)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def transform(self, X):
    return X.todense()</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="NERD" href="index.html">NERD</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="NERD.TEXT.BaseTextClassifier" href="#NERD.TEXT.BaseTextClassifier">BaseTextClassifier</a></code></h4>
<ul class="">
<li><code><a title="NERD.TEXT.BaseTextClassifier.add_unlabelled_examples" href="#NERD.TEXT.BaseTextClassifier.add_unlabelled_examples">add_unlabelled_examples</a></code></li>
<li><code><a title="NERD.TEXT.BaseTextClassifier.get_new_random_example" href="#NERD.TEXT.BaseTextClassifier.get_new_random_example">get_new_random_example</a></code></li>
<li><code><a title="NERD.TEXT.BaseTextClassifier.load_data" href="#NERD.TEXT.BaseTextClassifier.load_data">load_data</a></code></li>
<li><code><a title="NERD.TEXT.BaseTextClassifier.query_new_example" href="#NERD.TEXT.BaseTextClassifier.query_new_example">query_new_example</a></code></li>
<li><code><a title="NERD.TEXT.BaseTextClassifier.save_data" href="#NERD.TEXT.BaseTextClassifier.save_data">save_data</a></code></li>
<li><code><a title="NERD.TEXT.BaseTextClassifier.save_example" href="#NERD.TEXT.BaseTextClassifier.save_example">save_example</a></code></li>
<li><code><a title="NERD.TEXT.BaseTextClassifier.update_model" href="#NERD.TEXT.BaseTextClassifier.update_model">update_model</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="NERD.TEXT.ColumnsSelector" href="#NERD.TEXT.ColumnsSelector">ColumnsSelector</a></code></h4>
<ul class="">
<li><code><a title="NERD.TEXT.ColumnsSelector.fit" href="#NERD.TEXT.ColumnsSelector.fit">fit</a></code></li>
<li><code><a title="NERD.TEXT.ColumnsSelector.transform" href="#NERD.TEXT.ColumnsSelector.transform">transform</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="NERD.TEXT.DefaultTextFeaturizer" href="#NERD.TEXT.DefaultTextFeaturizer">DefaultTextFeaturizer</a></code></h4>
<ul class="">
<li><code><a title="NERD.TEXT.DefaultTextFeaturizer.fit" href="#NERD.TEXT.DefaultTextFeaturizer.fit">fit</a></code></li>
<li><code><a title="NERD.TEXT.DefaultTextFeaturizer.transform" href="#NERD.TEXT.DefaultTextFeaturizer.transform">transform</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="NERD.TEXT.DropColumns" href="#NERD.TEXT.DropColumns">DropColumns</a></code></h4>
<ul class="">
<li><code><a title="NERD.TEXT.DropColumns.fit" href="#NERD.TEXT.DropColumns.fit">fit</a></code></li>
<li><code><a title="NERD.TEXT.DropColumns.transform" href="#NERD.TEXT.DropColumns.transform">transform</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="NERD.TEXT.MultiLabelEncoder" href="#NERD.TEXT.MultiLabelEncoder">MultiLabelEncoder</a></code></h4>
<ul class="">
<li><code><a title="NERD.TEXT.MultiLabelEncoder.fit" href="#NERD.TEXT.MultiLabelEncoder.fit">fit</a></code></li>
<li><code><a title="NERD.TEXT.MultiLabelEncoder.transform" href="#NERD.TEXT.MultiLabelEncoder.transform">transform</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="NERD.TEXT.TextClassifier" href="#NERD.TEXT.TextClassifier">TextClassifier</a></code></h4>
<ul class="">
<li><code><a title="NERD.TEXT.TextClassifier.add_unlabelled_examples" href="#NERD.TEXT.TextClassifier.add_unlabelled_examples">add_unlabelled_examples</a></code></li>
<li><code><a title="NERD.TEXT.TextClassifier.load_labelled_examples" href="#NERD.TEXT.TextClassifier.load_labelled_examples">load_labelled_examples</a></code></li>
<li><code><a title="NERD.TEXT.TextClassifier.load_model" href="#NERD.TEXT.TextClassifier.load_model">load_model</a></code></li>
<li><code><a title="NERD.TEXT.TextClassifier.save_labelled_examples" href="#NERD.TEXT.TextClassifier.save_labelled_examples">save_labelled_examples</a></code></li>
<li><code><a title="NERD.TEXT.TextClassifier.save_model" href="#NERD.TEXT.TextClassifier.save_model">save_model</a></code></li>
<li><code><a title="NERD.TEXT.TextClassifier.start_server" href="#NERD.TEXT.TextClassifier.start_server">start_server</a></code></li>
<li><code><a title="NERD.TEXT.TextClassifier.update_model" href="#NERD.TEXT.TextClassifier.update_model">update_model</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="NERD.TEXT.ToDense" href="#NERD.TEXT.ToDense">ToDense</a></code></h4>
<ul class="">
<li><code><a title="NERD.TEXT.ToDense.fit" href="#NERD.TEXT.ToDense.fit">fit</a></code></li>
<li><code><a title="NERD.TEXT.ToDense.transform" href="#NERD.TEXT.ToDense.transform">transform</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.7.4</a>.</p>
</footer>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad()</script>
</body>
</html>